{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTT7Wo7nXNQT",
        "outputId": "330fbe31-9fe5-46bb-9c23-ee2f3953031b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Mn44aJbbfCeg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading/Preprocessing\n",
        "color_data = pd.read_csv(\"color_calibration_formatted_data.csv\")\n",
        "# Drop NA Values from input and output columns\n",
        "color_data = color_data.dropna(subset=[\"True R\",\"True G\",\"True B\",\"Observed R\",\"Observed G\",\"Observed B\",\n",
        "                                       \"Red R\",\"Red G\",\"Red B\",\"Green R\",\"Green G\",\"Green B\",\"Blue R\",\n",
        "                                       \"Blue G\",\"Blue B\"])\n",
        "# Each sample has a Red, Blue, and Green color calibration, each with R, G, B values\n",
        "# If Red R < Red G or Red R < Red B, it is an anomalous sample and should be discarded, same with Green and Blue\n",
        "red_condition = (color_data['Red R'] > color_data['Red G']) & (color_data['Red R'] > color_data['Red B'])\n",
        "green_condition = (color_data['Green G'] > color_data['Green R']) & (color_data['Green G'] > color_data['Green B'])\n",
        "blue_condition = (color_data['Blue B'] > color_data['Blue R']) & (color_data['Blue B'] > color_data['Blue G'])\n",
        "color_data = color_data[red_condition & green_condition & blue_condition]\n",
        "color_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsPiUrXdfK63",
        "outputId": "3d0f5b99-967a-406c-a027-74c7a867bf14"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2844 entries, 0 to 3207\n",
            "Data columns (total 18 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Sample Name    52 non-null     object \n",
            " 1   Sample Number  2786 non-null   object \n",
            " 2   File Name      2760 non-null   object \n",
            " 3   True R         2844 non-null   int64  \n",
            " 4   True G         2844 non-null   int64  \n",
            " 5   True B         2844 non-null   int64  \n",
            " 6   Observed R     2844 non-null   float64\n",
            " 7   Observed G     2844 non-null   float64\n",
            " 8   Observed B     2844 non-null   float64\n",
            " 9   Red R          2844 non-null   float64\n",
            " 10  Red G          2844 non-null   float64\n",
            " 11  Red B          2844 non-null   float64\n",
            " 12  Green R        2844 non-null   float64\n",
            " 13  Green G        2844 non-null   float64\n",
            " 14  Green B        2844 non-null   float64\n",
            " 15  Blue R         2844 non-null   float64\n",
            " 16  Blue G         2844 non-null   float64\n",
            " 17  Blue B         2844 non-null   float64\n",
            "dtypes: float64(12), int64(3), object(3)\n",
            "memory usage: 422.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input values are Observed R,G,B, Red R,G,B, Blue R,G,B, Green R,G,B (12 values)\n",
        "X = color_data.iloc[:, 6:].values\n",
        "# Output values are True R,G,B\n",
        "y = color_data.iloc[:, 3:6].values\n",
        "# Get list of unique colors (each color is an list of [R,G,B]) which will be used for classification of predictions\n",
        "color_list = np.unique(y, axis=0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1q1Xi-7RfplX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a variety of Regression models\n",
        "model_list = [LinearRegression(), Ridge(alpha=0.5), DecisionTreeRegressor(max_depth=50), RandomForestRegressor(n_estimators=100)]\n",
        "model_names = [\"Linear Regression\", \"Ridge Regression\", \"Decision Tree\", \"Random Forest\", \"Neural Network\"]\n",
        "# Store the predictions of each model (on X_test) in prediction_list\n",
        "prediction_list = []\n",
        "for model in model_list:\n",
        "  model.fit(X_train, y_train)\n",
        "  prediction_list.append(model.predict(X_test))"
      ],
      "metadata": {
        "id": "4xZ7hnWUgtiH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a vanilla Neural Network\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 128) # Input layer (12) -> Hidden layer (128)\n",
        "        self.fc2 = nn.Linear(128, 64) # Hidden layer(128) -> Hidden layer (64)\n",
        "        self.fc3 = nn.Linear(64, 32)   # Hidden layer (64) -> Output layer (32)\n",
        "        self.fc4 = nn.Linear(32, 3)   # Hidden layer (32) -> Output layer (3)\n",
        "        self.relu = nn.ReLU()         # Activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)  # No activation in output layer (regression)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = NeuralNetwork()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the model\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:  # Print loss every 10 epochs\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Make predictions on X_test\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    prediction_list.append(y_pred_tensor.numpy())  # Convert predictions to numpy array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfnP0X3_RpLP",
        "outputId": "1e33acfe-f908-41f9-845b-76a5fce4bf7f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 28160.5996\n",
            "Epoch [20/1000], Loss: 8375.3281\n",
            "Epoch [30/1000], Loss: 3923.0496\n",
            "Epoch [40/1000], Loss: 2486.9375\n",
            "Epoch [50/1000], Loss: 1770.1331\n",
            "Epoch [60/1000], Loss: 1255.1582\n",
            "Epoch [70/1000], Loss: 963.4194\n",
            "Epoch [80/1000], Loss: 811.4290\n",
            "Epoch [90/1000], Loss: 709.8876\n",
            "Epoch [100/1000], Loss: 647.7502\n",
            "Epoch [110/1000], Loss: 606.1601\n",
            "Epoch [120/1000], Loss: 575.8665\n",
            "Epoch [130/1000], Loss: 553.2957\n",
            "Epoch [140/1000], Loss: 536.0654\n",
            "Epoch [150/1000], Loss: 522.4738\n",
            "Epoch [160/1000], Loss: 511.5196\n",
            "Epoch [170/1000], Loss: 502.6140\n",
            "Epoch [180/1000], Loss: 495.2282\n",
            "Epoch [190/1000], Loss: 488.8281\n",
            "Epoch [200/1000], Loss: 483.0999\n",
            "Epoch [210/1000], Loss: 477.6559\n",
            "Epoch [220/1000], Loss: 472.5848\n",
            "Epoch [230/1000], Loss: 467.7734\n",
            "Epoch [240/1000], Loss: 463.1546\n",
            "Epoch [250/1000], Loss: 458.6606\n",
            "Epoch [260/1000], Loss: 454.1432\n",
            "Epoch [270/1000], Loss: 449.5849\n",
            "Epoch [280/1000], Loss: 444.9786\n",
            "Epoch [290/1000], Loss: 440.2937\n",
            "Epoch [300/1000], Loss: 435.4306\n",
            "Epoch [310/1000], Loss: 430.2717\n",
            "Epoch [320/1000], Loss: 424.6662\n",
            "Epoch [330/1000], Loss: 418.5430\n",
            "Epoch [340/1000], Loss: 411.7079\n",
            "Epoch [350/1000], Loss: 403.9618\n",
            "Epoch [360/1000], Loss: 395.0654\n",
            "Epoch [370/1000], Loss: 384.7599\n",
            "Epoch [380/1000], Loss: 372.8093\n",
            "Epoch [390/1000], Loss: 359.2501\n",
            "Epoch [400/1000], Loss: 344.4093\n",
            "Epoch [410/1000], Loss: 329.2809\n",
            "Epoch [420/1000], Loss: 315.2003\n",
            "Epoch [430/1000], Loss: 303.1909\n",
            "Epoch [440/1000], Loss: 293.5082\n",
            "Epoch [450/1000], Loss: 285.4596\n",
            "Epoch [460/1000], Loss: 278.3036\n",
            "Epoch [470/1000], Loss: 271.7295\n",
            "Epoch [480/1000], Loss: 265.6405\n",
            "Epoch [490/1000], Loss: 260.1477\n",
            "Epoch [500/1000], Loss: 255.2769\n",
            "Epoch [510/1000], Loss: 251.0287\n",
            "Epoch [520/1000], Loss: 247.3080\n",
            "Epoch [530/1000], Loss: 244.0465\n",
            "Epoch [540/1000], Loss: 241.2300\n",
            "Epoch [550/1000], Loss: 238.8128\n",
            "Epoch [560/1000], Loss: 236.7251\n",
            "Epoch [570/1000], Loss: 234.9178\n",
            "Epoch [580/1000], Loss: 233.3138\n",
            "Epoch [590/1000], Loss: 231.8955\n",
            "Epoch [600/1000], Loss: 230.6407\n",
            "Epoch [610/1000], Loss: 229.5181\n",
            "Epoch [620/1000], Loss: 228.4738\n",
            "Epoch [630/1000], Loss: 227.5027\n",
            "Epoch [640/1000], Loss: 226.5809\n",
            "Epoch [650/1000], Loss: 225.7201\n",
            "Epoch [660/1000], Loss: 224.8844\n",
            "Epoch [670/1000], Loss: 224.0757\n",
            "Epoch [680/1000], Loss: 223.2885\n",
            "Epoch [690/1000], Loss: 222.5241\n",
            "Epoch [700/1000], Loss: 221.7543\n",
            "Epoch [710/1000], Loss: 220.9905\n",
            "Epoch [720/1000], Loss: 220.2503\n",
            "Epoch [730/1000], Loss: 219.5147\n",
            "Epoch [740/1000], Loss: 218.7802\n",
            "Epoch [750/1000], Loss: 218.0353\n",
            "Epoch [760/1000], Loss: 217.3020\n",
            "Epoch [770/1000], Loss: 216.6136\n",
            "Epoch [780/1000], Loss: 215.9302\n",
            "Epoch [790/1000], Loss: 215.2607\n",
            "Epoch [800/1000], Loss: 214.5989\n",
            "Epoch [810/1000], Loss: 213.9473\n",
            "Epoch [820/1000], Loss: 213.2991\n",
            "Epoch [830/1000], Loss: 212.6593\n",
            "Epoch [840/1000], Loss: 212.0177\n",
            "Epoch [850/1000], Loss: 211.3603\n",
            "Epoch [860/1000], Loss: 210.7063\n",
            "Epoch [870/1000], Loss: 210.0400\n",
            "Epoch [880/1000], Loss: 209.3722\n",
            "Epoch [890/1000], Loss: 208.6817\n",
            "Epoch [900/1000], Loss: 208.0079\n",
            "Epoch [910/1000], Loss: 207.3386\n",
            "Epoch [920/1000], Loss: 206.6973\n",
            "Epoch [930/1000], Loss: 206.0924\n",
            "Epoch [940/1000], Loss: 205.5045\n",
            "Epoch [950/1000], Loss: 204.9291\n",
            "Epoch [960/1000], Loss: 204.3661\n",
            "Epoch [970/1000], Loss: 203.8051\n",
            "Epoch [980/1000], Loss: 203.1879\n",
            "Epoch [990/1000], Loss: 202.5922\n",
            "Epoch [1000/1000], Loss: 202.0073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the accuracy of each model (number of correctly classified samples in X_test/len(X_test)) in accuracy_list\n",
        "accuracy_list = []\n",
        "for predictions in prediction_list:\n",
        "  # A corrected prediction is the closest color (from True R,G,B) to a prediction\n",
        "  corrected_preds = []\n",
        "  for pred in predictions:\n",
        "    # Find the closest color to each prediction (Euclidean distance measurement)\n",
        "    min_dist = np.linalg.norm(pred - color_list[0])\n",
        "    closest_color = color_list[0]\n",
        "    for color_vec in color_list:\n",
        "      if np.linalg.norm(pred - color_vec) < min_dist:\n",
        "        min_dist = np.linalg.norm(pred - color_vec)\n",
        "        closest_color = color_vec\n",
        "    corrected_preds.append(closest_color)\n",
        "  corrected_preds = np.array(corrected_preds)\n",
        "\n",
        "  # Find the number of correctly classified samples\n",
        "  correct = 0\n",
        "  for i in range(len(y_test)):\n",
        "    if y_test[i][0] == corrected_preds[i][0] and y_test[i][1] == corrected_preds[i][1] and y_test[i][2] == corrected_preds[i][2]:\n",
        "      correct += 1\n",
        "\n",
        "  accuracy_list.append(correct/len(y_test))"
      ],
      "metadata": {
        "id": "GGHtf5PU7wWH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the [MSE, RMSE] for each model in mse_list\n",
        "mse_list = []\n",
        "for predictions in prediction_list:\n",
        "  mse = mean_squared_error(y_test, predictions)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mse_list.append([mse, rmse])\n",
        "\n",
        "# Print out the accuracy and MSE for each model\n",
        "for i in range(len(model_names)):\n",
        "  print(\"Model:\", model_names[i])\n",
        "  print(\"Classification Accuracy:\", accuracy_list[i])\n",
        "  print(\"MSE:\", mse_list[i][0])\n",
        "  print(\"RMSE:\", mse_list[i][1])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eG5m_-xhCRM",
        "outputId": "93add42c-ba35-4d21-dddb-f0d5496203c8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Linear Regression\n",
            "Classification Accuracy: 0.16520210896309315\n",
            "MSE: 329.99565123815006\n",
            "RMSE: 18.16578242846011\n",
            "\n",
            "Model: Ridge Regression\n",
            "Classification Accuracy: 0.1634446397188049\n",
            "MSE: 330.0301685806478\n",
            "RMSE: 18.16673246846135\n",
            "\n",
            "Model: Decision Tree\n",
            "Classification Accuracy: 0.45869947275922673\n",
            "MSE: 391.9326303456356\n",
            "RMSE: 19.79728845942382\n",
            "\n",
            "Model: Random Forest\n",
            "Classification Accuracy: 0.4270650263620387\n",
            "MSE: 218.51717844171048\n",
            "RMSE: 14.782326557132693\n",
            "\n",
            "Model: Neural Network\n",
            "Classification Accuracy: 0.1968365553602812\n",
            "MSE: 204.3705596923828\n",
            "RMSE: 14.295823155466874\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search Hyperparameter Optimization on Random Forest Regressor\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [15, 30, 60],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=RandomForestRegressor(),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best parameters for Random Forest Regressor:\", best_params)\n",
        "\n",
        "predictions = best_model.predict(X_test)\n",
        "# A corrected prediction is the closest color (from True R,G,B) to a prediction\n",
        "corrected_preds = []\n",
        "for pred in predictions:\n",
        "  # Find the closest color to each prediction (Euclidean distance measurement)\n",
        "  min_dist = np.linalg.norm(pred - color_list[0])\n",
        "  closest_color = color_list[0]\n",
        "  for color_vec in color_list:\n",
        "    if np.linalg.norm(pred - color_vec) < min_dist:\n",
        "      min_dist = np.linalg.norm(pred - color_vec)\n",
        "      closest_color = color_vec\n",
        "  corrected_preds.append(closest_color)\n",
        "corrected_preds = np.array(corrected_preds)\n",
        "\n",
        "# Find the number of correctly classified samples\n",
        "correct = 0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i][0] == corrected_preds[i][0] and y_test[i][1] == corrected_preds[i][1] and y_test[i][2] == corrected_preds[i][2]:\n",
        "    correct += 1\n",
        "\n",
        "print(\"Accuracy of Best Random Forest Regressor:\", correct/len(y_test))\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"MSE of Best Random Forest Regressor:\", mse)\n",
        "print(\"RMSE of Best Random Forest Regressor:\", rmse)"
      ],
      "metadata": {
        "id": "45Nt12wN7g2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd984589-cc0c-406e-84e9-8397f088bb4c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest Regressor: {'max_depth': 60, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
            "Accuracy of Best Random Forest Regressor: 0.4358523725834798\n",
            "MSE of Best Random Forest Regressor: 214.5185299988284\n",
            "RMSE of Best Random Forest Regressor: 14.646451105944688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search Hyperparameter Optimization on Decision Tree\n",
        "param_grid = {\n",
        "    'criterion': ['absolute_error', 'squared_error', 'poisson', 'friedman_mse'],\n",
        "    'max_depth': [15, 30, 60, 100],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 5]\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeRegressor(),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best parameters for Decision Tree Regressor:\", best_params)\n",
        "\n",
        "predictions = best_model.predict(X_test)\n",
        "# A corrected prediction is the closest color (from True R,G,B) to a prediction\n",
        "corrected_preds = []\n",
        "for pred in predictions:\n",
        "  # Find the closest color to each prediction (Euclidean distance measurement)\n",
        "  min_dist = np.linalg.norm(pred - color_list[0])\n",
        "  closest_color = color_list[0]\n",
        "  for color_vec in color_list:\n",
        "    if np.linalg.norm(pred - color_vec) < min_dist:\n",
        "      min_dist = np.linalg.norm(pred - color_vec)\n",
        "      closest_color = color_vec\n",
        "  corrected_preds.append(closest_color)\n",
        "corrected_preds = np.array(corrected_preds)\n",
        "\n",
        "# Find the number of correctly classified samples\n",
        "correct = 0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i][0] == corrected_preds[i][0] and y_test[i][1] == corrected_preds[i][1] and y_test[i][2] == corrected_preds[i][2]:\n",
        "    correct += 1\n",
        "\n",
        "print(\"Accuracy of Best Decision Tree Regressor:\", correct/len(y_test))\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"MSE of Best Decision Tree Regressor:\", mse)\n",
        "print(\"RMSE of Best Decision Tree Regressor:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMOJUcP0poE-",
        "outputId": "ba661db4-3459-4dae-ca00-9edc00e21ee1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Decision Tree Regressor: {'criterion': 'squared_error', 'max_depth': 30, 'min_samples_leaf': 5, 'min_samples_split': 5}\n",
            "Accuracy of Best Decision Tree Regressor: 0.3181019332161687\n",
            "MSE of Best Decision Tree Regressor: 317.8951542312287\n",
            "RMSE of Best Decision Tree Regressor: 17.82961452839709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid Search Hyperparameter Optimization on Neural Network\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Define the neural network with tunable hyperparameters\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # No activation in the output layer (for regression)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    hidden_size1 = trial.suggest_int(\"hidden_size1\", 32, 128, step=16)\n",
        "    hidden_size2 = trial.suggest_int(\"hidden_size2\", 16, 64, step=16)\n",
        "    lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    # Create DataLoader for mini-batch training\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = NeuralNetwork(input_size=12, hidden_size1=hidden_size1, hidden_size2=hidden_size2, output_size=3)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Train the model\n",
        "    epochs = 1000\n",
        "    for epoch in range(epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor)\n",
        "        test_loss = criterion(y_pred, y_test_tensor).item()\n",
        "\n",
        "    return test_loss  # Minimize the loss\n",
        "\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = study.best_params\n",
        "print(\"\\nBest Hyperparameters:\", best_params)\n",
        "\n",
        "# Train final model with best parameters\n",
        "best_model = NeuralNetwork(\n",
        "    input_size=12,\n",
        "    hidden_size1=best_params[\"hidden_size1\"],\n",
        "    hidden_size2=best_params[\"hidden_size2\"],\n",
        "    output_size=3\n",
        ")\n",
        "best_optimizer = optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(1000):\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        best_optimizer.zero_grad()\n",
        "        outputs = best_model(batch_X)\n",
        "        loss = nn.MSELoss()(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        best_optimizer.step()\n",
        "\n",
        "# Predict using the best model\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = best_model(X_test_tensor)\n",
        "    predictions = y_pred_tensor.numpy()\n",
        "\n",
        "# A corrected prediction is the closest color (from True R,G,B) to a prediction\n",
        "corrected_preds = []\n",
        "for pred in predictions:\n",
        "  # Find the closest color to each prediction (Euclidean distance measurement)\n",
        "  min_dist = np.linalg.norm(pred - color_list[0])\n",
        "  closest_color = color_list[0]\n",
        "  for color_vec in color_list:\n",
        "    if np.linalg.norm(pred - color_vec) < min_dist:\n",
        "      min_dist = np.linalg.norm(pred - color_vec)\n",
        "      closest_color = color_vec\n",
        "  corrected_preds.append(closest_color)\n",
        "corrected_preds = np.array(corrected_preds)\n",
        "\n",
        "# Find the number of correctly classified samples\n",
        "correct = 0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i][0] == corrected_preds[i][0] and y_test[i][1] == corrected_preds[i][1] and y_test[i][2] == corrected_preds[i][2]:\n",
        "    correct += 1\n",
        "\n",
        "print(\"Accuracy of Best Neural Network:\", correct/len(y_test))\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"MSE of Best Neural Network:\", mse)\n",
        "print(\"RMSE of Best Neural Network:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6sK7VXrJJs0",
        "outputId": "995f990f-7bb3-48bf-db14-260a82df3d43"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-24 04:20:40,911] A new study created in memory with name: no-name-553cf5d6-e1de-4c78-837c-405aa4e95d4b\n",
            "<ipython-input-54-00d717609321>:36: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
            "[I 2025-03-24 04:22:47,563] Trial 0 finished with value: 213.05992126464844 and parameters: {'hidden_size1': 32, 'hidden_size2': 48, 'lr': 0.0002047694263188294, 'batch_size': 32}. Best is trial 0 with value: 213.05992126464844.\n",
            "[I 2025-03-24 04:26:28,669] Trial 1 finished with value: 165.5153350830078 and parameters: {'hidden_size1': 48, 'hidden_size2': 32, 'lr': 0.0006987412679126773, 'batch_size': 16}. Best is trial 1 with value: 165.5153350830078.\n",
            "[I 2025-03-24 04:30:11,691] Trial 2 finished with value: 137.11451721191406 and parameters: {'hidden_size1': 48, 'hidden_size2': 32, 'lr': 0.006494350317648316, 'batch_size': 16}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:31:27,582] Trial 3 finished with value: 251.0258331298828 and parameters: {'hidden_size1': 48, 'hidden_size2': 32, 'lr': 0.00018098922737104202, 'batch_size': 64}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:32:41,582] Trial 4 finished with value: 172.00265502929688 and parameters: {'hidden_size1': 32, 'hidden_size2': 48, 'lr': 0.0010579944982916187, 'batch_size': 64}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:36:30,627] Trial 5 finished with value: 183.83721923828125 and parameters: {'hidden_size1': 128, 'hidden_size2': 32, 'lr': 0.00011207749975117733, 'batch_size': 16}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:40:08,952] Trial 6 finished with value: 193.4805908203125 and parameters: {'hidden_size1': 32, 'hidden_size2': 48, 'lr': 0.009826393691910046, 'batch_size': 16}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:42:13,503] Trial 7 finished with value: 169.4169921875 and parameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.002199491721736195, 'batch_size': 32}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:45:52,731] Trial 8 finished with value: 177.6080780029297 and parameters: {'hidden_size1': 32, 'hidden_size2': 64, 'lr': 0.000408793386249092, 'batch_size': 16}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:47:53,777] Trial 9 finished with value: 209.0513153076172 and parameters: {'hidden_size1': 32, 'hidden_size2': 48, 'lr': 0.000169092041142411, 'batch_size': 32}. Best is trial 2 with value: 137.11451721191406.\n",
            "[I 2025-03-24 04:51:32,817] Trial 10 finished with value: 130.20066833496094 and parameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.009680958909769614, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 04:55:13,908] Trial 11 finished with value: 133.35000610351562 and parameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.005921348188044645, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 04:58:55,728] Trial 12 finished with value: 156.18821716308594 and parameters: {'hidden_size1': 96, 'hidden_size2': 16, 'lr': 0.003680028193500232, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:02:35,948] Trial 13 finished with value: 143.77880859375 and parameters: {'hidden_size1': 96, 'hidden_size2': 16, 'lr': 0.003612947277948799, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:03:47,592] Trial 14 finished with value: 167.97056579589844 and parameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.009881892180366657, 'batch_size': 64}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:07:30,949] Trial 15 finished with value: 149.67453002929688 and parameters: {'hidden_size1': 112, 'hidden_size2': 16, 'lr': 0.0019259101520640656, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:11:15,949] Trial 16 finished with value: 163.36412048339844 and parameters: {'hidden_size1': 64, 'hidden_size2': 64, 'lr': 0.0048177188065800566, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:14:54,193] Trial 17 finished with value: 161.59295654296875 and parameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.002123918023534869, 'batch_size': 16}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:17:01,219] Trial 18 finished with value: 190.6936492919922 and parameters: {'hidden_size1': 96, 'hidden_size2': 32, 'lr': 0.00666574222286537, 'batch_size': 32}. Best is trial 10 with value: 130.20066833496094.\n",
            "[I 2025-03-24 05:18:12,940] Trial 19 finished with value: 191.5281524658203 and parameters: {'hidden_size1': 64, 'hidden_size2': 16, 'lr': 0.0009424433548722497, 'batch_size': 64}. Best is trial 10 with value: 130.20066833496094.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters: {'hidden_size1': 80, 'hidden_size2': 16, 'lr': 0.009680958909769614, 'batch_size': 16}\n",
            "Accuracy of Best Neural Network: 0.36203866432337434\n",
            "MSE of Best Neural Network: 175.67047119140625\n",
            "RMSE of Best Neural Network: 13.25407375833582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdl_IKdUYZ1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}